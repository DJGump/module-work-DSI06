{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may recall from the *Model preparation* module earlier in this program, a typical data science workflow (or pipeline) looks like this:\n",
    "\n",
    "![Data Science Workflow](assets/eda.png)\n",
    "\n",
    "When you're working with text, you need to modify the above process to account for the peculiarities of text data. Look at the modified diagram below:\n",
    "\n",
    "![Data Preprocessing](assets/data_preprocessing.png)\n",
    "\n",
    "As you can see, the three steps in the exploratory data analysis process still occur in the same sequence. But the feature-engineering step is modified slightly. Now, you'll go step by step through the data science workflow for working with text data:\n",
    "\n",
    "1. **Data cleaning:** Clean your data to remove the problematic parts. In this checkpoint, you will walk through the basic steps of data cleaning for text data.\n",
    "\n",
    "2. **Data exploration:** Do some data exploration to get to know your data better. In this module, there won't be a separate checkpoint dedicated to this step. This is mainly because the data exploration checkpoints of the *Model preparation* module covered many techniques that you can also apply to text data.\n",
    "\n",
    "3. **Feature engineering:** The last step in data preprocessing is preparing the features that you'll use in the modeling phase. In this respect, text data is special, and it requires different methods for creating features that will be used in the modeling. In this step, you'll convert text data into numerical form; this process is also known as *language modeling*. The next three checkpoints focus on this step.\n",
    "\n",
    "Note that the steps above are iterative, and you may need to go back when you feel it's necessary.\n",
    "\n",
    "Once you've prepared your numerical features, then you can jump into the modeling phase and apply machine-learning techniques for different supervised and unsupervised tasks.\n",
    "\n",
    "In this checkpoint, you'll learn about some data-cleaning methods for text data. You'll focus on methods that are specific to text data. Hence, you'll skip the common methods that are also relevant to numerical data; those techniques are covered in the *Model preparation* module. With this checkpoint, you'll also be introduced to the tools that you'll be using throughout the module. Specifically, you'll make use of two NLP packages: NLTK and spaCy. \n",
    "\n",
    "* As touched upon previously, NLTK is a seasoned package with rich functionality. It is highly customizable and contains many models that are useful for learning NLP but are no longer state of the art and aren't optimal for production code. \n",
    "\n",
    "* spaCy is almost the direct opposite. It doesn't offer many models; rather, it uses algorithms and methods that are currently considered state of the art. Note that using only a limited number of algorithms and models generates a tradeoff between flexibility and ease of use. And spaCy optimizes for ease of use rather than flexibilityâ€”which makes its usage much leaner. Furthermore, it's written in *Cython* (that is, the Python code that you write is translated into C), so it's considerably faster. Indeed, it's among the fastest NLP libraries available.\n",
    "\n",
    "In the rest of this module, the functionalities of NLTK and spaCy will be used interchangeably. Some functionalities are available in both packages, but you'll usually use a functionality from a single package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for text preprocessing\n",
    "\n",
    "Most of the time, you'll start with a raw dataset consisting of many documents. But in order to work on them, you need to organize the material in a convenient way. You also need to clean the noisy and problematic parts that can harm your analysis or increase computation time without any informational gain.\n",
    "\n",
    "## Preparing the dataset\n",
    "\n",
    "Organizing the material means preparing a dataset that you want to work with. Specifically, you want a tabular dataset where rows represent the observations and columns represent the features. For example, imagine that you have a dataset that consists of articles written by several authors, and your goal is to distinguish authors from their writing styles. In that case, you would want the articles and the associated authors to be represented in rows, with the article and the author stored in two columns. However, this is just an example; there are many possible use cases. Here are some other examples:\n",
    "\n",
    "* If your dataset consists of books with multiple chapters, and each chapter may be written by a different author, then you may need to represent chapters as rows.\n",
    "\n",
    "* If your aim is to summarize the articles, then you may need to derive the paragraphs from the articles and make each paragraph a row.\n",
    "\n",
    "* If you want to translate a language into another one, then you may need to prepare a dataset where two sentences from the two languages form a row.\n",
    "\n",
    "So, building up a dataset from a raw material depends on the goal that you want to achieve with that dataset. \n",
    "\n",
    "In the following three checkpoints, you'll prepare your dataset in the manner highlighted above. For now, you'll clean your data.\n",
    "\n",
    "## Cleaning the dataset\n",
    "\n",
    "To get your dataset into the format that you need, start by cleaning your dataset. When it comes to text data, *cleaning* means a lot of things, and you shouldn't consider it too narrowly. Here are some common steps for cleaning text data:\n",
    "\n",
    "* Correcting any typos and misspelled words\n",
    "\n",
    "* Dealing with abbreviations\n",
    "\n",
    "* Making all characters lowercase (or uppercase)\n",
    "\n",
    "* Removing any emojis\n",
    "\n",
    "* Removing the stop words\n",
    "\n",
    "* Normalizing the words (also known as lemmatization or stemming)\n",
    "\n",
    "This isn't a complete list; there are several other items that could be added to this list. In this checkpoint, you'll focus on removing the stop words and normalizing the text. But if you want to learn more about common data-cleaning steps, check out Kaggle's [Getting started with Text Preprocessing](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing in action\n",
    "\n",
    "Now that you've reviewed why you need to preprocess the text data, you can begin to work on a dataset using NLTK and spaCy. First, if you haven't installed NLTK and spaCy yet, install the packages now using the following code:\n",
    "\n",
    "```bash \n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```bash \n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "You'll also need to download the English model of spaCy from your command prompt (or Terminal, if you're on a Mac) using this code:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en\n",
    "```\n",
    "Or, more conveniently, you can do the same inside Jupyter Notebook by running the following code in a cell:\n",
    "\n",
    "```bash\n",
    "!python -m spacy download en\n",
    "```\n",
    "\n",
    "As usual, begin with importing the libraries that you'll use in this checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.44.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dgump\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.2.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "[x] Couldn't link model to 'en'\n",
      "Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "permissions and try re-running the command as admin, or use a virtualenv. You\n",
      "can still import the model as a module and call its load() method, or create the\n",
      "symlink manually.\n",
      "C:\\Users\\dgump\\miniconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\dgump\\miniconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "[!] Download successful but linking failed\n",
      "Creating a shortcut link for 'en' didn't work (maybe you don't have admin\n",
      "permissions?), but you can still load the model via its full package name: nlp =\n",
      "spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You do not have sufficient privilege to perform this operation.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download the data that you'll be using from NLTK's repository. Specifically, use NLTK's Gutenberg corpus, which is a sample from the [Gutenberg Project](https://www.gutenberg.org/). The project itself is a large collection of books written in English.\n",
    "\n",
    "Notice that installing NLTK isn't sufficient to make these corpora available for your use. You need to download them using `nlt.download()` by providing the corpus name as a parameter. Use the following code to download this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\dgump\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch the installer to download the Gutenberg corpus\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "# Download the English models of spaCy\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There's also an interactive way to download data from the NLTK repository. To use this method instead, run the following: \n",
    "```python\n",
    "nltk.download()\n",
    "```\n",
    "This will launch an [interactive installer](http://www.nltk.org/data.html#interactive-installer). Using the installer, you can choose the **Corpora** tab and download the Gutenberg corpus.\n",
    "\n",
    "Now that you've acquired some data, dig in to look at it and get ready to clean things up. You'll work with two novels from the Gutenberg corpus: *Alice's Adventures in Wonderland* by Lewis Carroll, and *Persuasion* by Jane Austen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "\n",
      "Raw:\n",
      " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "# Import the data that you just downloaded\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Grab and process the raw data\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# Print the first 100 characters of Alice\n",
    "print('\\nRaw:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text cleaning\n",
    "\n",
    "First, do some basic data cleaning in your data. Note that this kind of cleaning depends very much on the corpus that you're working on. This is because all texts have their own peculiarities that need to be dealt with. As you'll see shortly, you'll remove some strings that are specific to the texts that you're using.\n",
    "\n",
    "For modifying text data, using *regular expressions* is a common practice. You'll also use regular expressions (specifically [`re.sub()`](https://docs.python.org/3/library/re.html#re.sub), short for *substitute*) to identify and remove substrings that you don't want. Specifically, you'll match those substrings with a regular expression and substitute in an empty string for them.\n",
    "\n",
    "This checkpoint won't go into detail about how regular expressions work, but you should be able to get a good sense of what's happening by reading the code. If you want more information, Python's [Regular Expression HOWTO](https://docs.python.org/3/howto/regex.html) is an accessible starting point and reference. And [RegExr](http://regexr.com/) is a useful tool for visualizing and tinkering with regular expressions.\n",
    "\n",
    "Start your cleaning by removing the title. You'll match all text between square brackets and replace it with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title removed: \n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "# This pattern matches all text between square brackets\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "\n",
    "# Print the first 100 characters of Alice again\n",
    "print(\"Title removed:\", alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove the chapter headings like `CHAPTER I`. Note that the two novels have different styles of chapter headings. So, you'll deal with each novel one by one. This is quite typical in cleaning text data. As mentioned before, all texts have their own peculiarities, and cleaning them requires you to know those peculiarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter headings removed: \n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothin\n"
     ]
    }
   ],
   "source": [
    "# Now, match and remove chapter headings\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "# Okay, what does it look like now?\n",
    "print('Chapter headings removed:', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to read the two novels, you'd notice that there are a lot of newline characters and other types of extra whitespace. So, you need to clean them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed: Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to\n"
     ]
    }
   ],
   "source": [
    "# Remove newlines and other extra whitespace by splitting and rejoining\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "# All done with cleanup? See how it looks.\n",
    "print('Extra whitespace removed:', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data-cleaning steps that you've gone through so far should give you an idea of the kinds of problems that you may encounter in a corpus. As you can imagine, you may encounter many more types of problems than you've covered here. For example, if you were to work on a social media corpus, then you would most likely encounter many emojis and abbreviations. Dealing with those would also be a major problem in the data-cleaning phase. So you should always pay attention to what kind of corpus you have and what types of problems may occur in the text.\n",
    "\n",
    "Now that your text is starting to look okay, the next step is to tokenize your texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "As you may recall from the previous checkpoint, each individual meaningful piece from a text is called a *token*, and the process of breaking up the text into these pieces is called *tokenization*. Tokenization is an important step in text preprocessing; most of the time, you'll generate the numerical representations of your texts from these tokens. Hence, correctly breaking up the text into tokens is a crucial step for the success of the next steps of any data science workflow.\n",
    "\n",
    "Tokens are generally words and punctuation. In some NLP applications, people remove the punctuation marks from the text along with the stop words. There's no single correct way to handle punctuation marks, and it's usually a matter of experimentation to determine the best approach. Here, you'll keep punctuation marks in your documents for now, because you'll make use of them when you're separating your text into sentences. But when you analyze your data, you'll check for them and exclude them from your analysis, as you'll see shortly.\n",
    "\n",
    "Go ahead and use spaCy to parse your novels into tokens. When you call spaCy on the novel, it will immediately and automatically parse it, tokenizing the string by breaking it into words and punctuation (and many other things that you will explore):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "# All the processing work takes place below, so it may take a while\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of your parsed documents are now stored in two defined variables. spaCy did a lot of good things when parsing the documents. Now, see what you have after the parsing happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alice_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34408 tokens long\n",
      "The first three tokens are 'Alice was beginning'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Explore the objects that you've built.\n",
    "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from inspecting the spaCy objects above that you're playing around with [`doc`](https://spacy.io/docs/api/doc) and [`token`](https://spacy.io/docs/api/token) objects. These are the types that are defined by spaCy.\n",
    "\n",
    "Now that you've parsed your documents and extracted the tokens, you can remove the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Removing stop words from the dataset is an important step in text preprocessing. This is because stop words occur frequently in the textâ€”and most of the time, they convey little meaning. So removing them delivers two benefits:\n",
    "\n",
    "1. You get rid of noisy data.\n",
    "2. The size of the text diminishes, so the computation time shortens.\n",
    "\n",
    "Removing stop words with spaCy is quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "alice_without_stopwords = [token for token in alice_doc if not token.is_stop]\n",
    "persuasion_without_stopwords = [token for token in persuasion_doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this code iterates over the tokens that are already made available by spaCy during the parsing of the documents, and it excludes the tokens that are stop words. \n",
    "\n",
    "Now, store your tokens in two lists that are free of stop words. Next, briefly stop text processing and examine how frequently each token appears in your corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('said', 453), ('Alice', 394), ('little', 124), ('like', 84), ('went', 83), ('know', 83), ('thought', 74), ('Queen', 73), ('time', 68), ('King', 61)]\n",
      "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 254), ('Wentworth', 217), ('Lady', 191), ('good', 181), ('little', 175), ('Charles', 166)]\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently words appear in the text\n",
    "def word_frequencies(text):\n",
    "    \n",
    "    # Build a list of words\n",
    "    # Strip out punctuation\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a `Counter` object containing word counts\n",
    "    return Counter(words)\n",
    "\n",
    "# Instantiate your list of the most common words\n",
    "alice_word_freq = word_frequencies(alice_without_stopwords).most_common(10)\n",
    "persuasion_word_freq = word_frequencies(persuasion_without_stopwords).most_common(10)\n",
    "print('\\nAlice:', alice_word_freq)\n",
    "print('Persuasion:', persuasion_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to think about the 10 most common words in each novel. Do you see some differences that make sense to you?\n",
    "\n",
    "After tokenization, the natural next step in text processing is lemmatization or stemming. Thinkful's data science team prefers to use lemmatization here, but you can also play with stemming if you like. Again, most of the time, determining if lemmatization or stemming is the best choice is a matter of experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "So far, you've tokenized your texts and looked at whether certain words are present and how frequently they appear. You can process these words further to remove a little more noise from your data. Consider the words `think`, `thought`, and `thinking`. They're related. They all share the same root word: the verb `think`. Most of the time, you'll want to focus on the fact that the act of thinking comes up a lot in data, rather than have that information split across all the different forms of `think`.\n",
    "\n",
    "To focus in like this, you can reduce each word to its root (that is, to its lemma) and do your counts again. This time, you're building a count of concepts rather than just words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('say', 477), ('Alice', 391), ('think', 130), ('go', 130), ('little', 126), ('look', 106), ('know', 103), ('come', 96), ('like', 92), ('begin', 91)]\n",
      "Persuasion: [('Anne', 496), ('Captain', 295), ('Mrs', 291), ('Elliot', 288), ('think', 256), ('Mr', 254), ('know', 252), ('good', 223), ('Wentworth', 216), ('say', 192)]\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently each lemma appears in the text\n",
    "def lemma_frequencies(text):\n",
    "    \n",
    "    # Build a list of lemmas\n",
    "    # Strip out punctuation\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a `Counter` object containing lemma counts\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate your list of most common lemmas\n",
    "alice_lemma_freq = lemma_frequencies(alice_without_stopwords).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_without_stopwords).most_common(10)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('Persuasion:', persuasion_lemma_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may realize, the top 10 list changed. You can try printing more of the top lemmas, which may help you catch meaningful differences between the two novels.\n",
    "\n",
    "Alternatively, you can identify the lemmas that are common to one text but not the other. This may help you to understand the differences between the two novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Alice: {'little', 'go', 'like', 'come', 'Alice', 'begin', 'look'}\n",
      "Unique to Persuasion: {'Mrs', 'Mr', 'Elliot', 'Anne', 'Wentworth', 'Captain', 'good'}\n"
     ]
    }
   ],
   "source": [
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "These are examples of data exploration on text data. When it comes to text data, the sky is the limit! So, use your imagination and consider more creative ways to analyze the two novels based on their lemmas.\n",
    "\n",
    "You won't go into the details here, but some syntactic properties can also help in this analysis. You may notice that the most frequently occurring lemmas include person names. For the purpose of your analysis, you may need to eliminate them from the lists. To do this, you can derive the named entities in the texts. And spaCy has already derived named entities in the texts during parsing. If you like, you can go ahead and inspect the named entities.\n",
    "\n",
    "**Note:** Your tokens are lemmatized to treat words with similar meanings as if they are the same. Apart from looking at lemmas, you could also perform a similar analysis by pulling out prefixes (`token.prefix_`) or suffixes (`token.suffix_`) from the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences\n",
    "\n",
    "Before finishing this checkpoint, take a moment to consider how to determine the sentences in a corpus. Beyond individual words, text can also be considered at the level of sentences. Using punctuation cues, you can split up text into sentences. Each sentence can then be summarized by, for example, using sentiment analysis to categorize sentences as having positive or negative sentiment. You may also be interested in how long sentences tend to be, or how many unique words make up a sentence. The sentence also provides context for the individual words, allowing you to draw even more information from each word.\n",
    "\n",
    "You get a lot of automatic sentence-level information from spaCy. The `doc.sents` property will give you each sentence as a [`span`](https://spacy.io/docs/api/span) object. Now, look at some of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1823 sentences.\n",
      "Here is an example: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences\n",
    "sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 words in this sentence, and 23 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "# Look at some metrics around this sentence\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, sentence-level analysis can also be helpful in the data exploration phase.\n",
    "\n",
    "Now, it's your turn to complete the assignments and apply what you've learned so far about data cleaning and text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
