{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCLo8OfaJeLJ"
   },
   "source": [
    "# Exploring and Analyzing Text Data Assignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"#!pip install wordcloud\";\n",
       "                var nbb_formatted_code = \"#!pip install wordcloud\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILXNGki2JeLM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.44.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dgump\\appdata\\roaming\\python\\python37\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dgump\\miniconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.2.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"import spacy\\nimport string\\nimport pandas as pd\\nfrom nltk import pos_tag\\nfrom nltk.text import Text\\nfrom nltk import sent_tokenize\\nfrom nltk import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom nltk.probability import FreqDist\\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom wordcloud import WordCloud, STOPWORDS\\n\\n!python -m spacy download en_core_web_sm\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\";\n",
       "                var nbb_formatted_code = \"import spacy\\nimport string\\nimport pandas as pd\\nfrom nltk import pos_tag\\nfrom nltk.text import Text\\nfrom nltk import sent_tokenize\\nfrom nltk import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom nltk.probability import FreqDist\\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom wordcloud import WordCloud, STOPWORDS\\n\\n!python -m spacy download en_core_web_sm\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.text import Text\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def text_stats(doc):\\n    sents = sent_tokenize(doc)\\n    tokens = word_tokenize(doc)\\n    words = [token.lower() for token in tokens \\n             if not token.lower() in stopwords.words('english')\\n             if not token in string.punctuation]\\n\\n    num_sents = len(sents)\\n    num_tokens = len(tokens)\\n    num_words = len(words)\\n    vocab = len(set(words))\\n    characters = sum([len(word) for word in words])\\n    \\n    spacy_doc = nlp(doc)\\n    remove = ['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', \\n              'ORDINAL', 'CARDINAL']\\n    entities = [entity.text for entity in spacy_doc.ents \\n                if not entity.label_ in remove]\\n\\n    num_entities = len(set(entities))\\n    words_sent = num_words / num_sents\\n    char_word = characters / num_words\\n    lex_div = vocab / num_words\\n    \\n    stats = [num_sents, num_tokens, num_words, vocab, num_entities, \\n             words_sent, char_word, lex_div]\\n\\n    return stats\";\n",
       "                var nbb_formatted_code = \"def text_stats(doc):\\n    sents = sent_tokenize(doc)\\n    tokens = word_tokenize(doc)\\n    words = [\\n        token.lower()\\n        for token in tokens\\n        if not token.lower() in stopwords.words(\\\"english\\\")\\n        if not token in string.punctuation\\n    ]\\n\\n    num_sents = len(sents)\\n    num_tokens = len(tokens)\\n    num_words = len(words)\\n    vocab = len(set(words))\\n    characters = sum([len(word) for word in words])\\n\\n    spacy_doc = nlp(doc)\\n    remove = [\\\"DATE\\\", \\\"TIME\\\", \\\"PERCENT\\\", \\\"MONEY\\\", \\\"QUANTITY\\\", \\\"ORDINAL\\\", \\\"CARDINAL\\\"]\\n    entities = [entity.text for entity in spacy_doc.ents if not entity.label_ in remove]\\n\\n    num_entities = len(set(entities))\\n    words_sent = num_words / num_sents\\n    char_word = characters / num_words\\n    lex_div = vocab / num_words\\n\\n    stats = [\\n        num_sents,\\n        num_tokens,\\n        num_words,\\n        vocab,\\n        num_entities,\\n        words_sent,\\n        char_word,\\n        lex_div,\\n    ]\\n\\n    return stats\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def text_stats(doc):\n",
    "    sents = sent_tokenize(doc)\n",
    "    tokens = word_tokenize(doc)\n",
    "    words = [token.lower() for token in tokens \n",
    "             if not token.lower() in stopwords.words('english')\n",
    "             if not token in string.punctuation]\n",
    "\n",
    "    num_sents = len(sents)\n",
    "    num_tokens = len(tokens)\n",
    "    num_words = len(words)\n",
    "    vocab = len(set(words))\n",
    "    characters = sum([len(word) for word in words])\n",
    "    \n",
    "    spacy_doc = nlp(doc)\n",
    "    remove = ['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', \n",
    "              'ORDINAL', 'CARDINAL']\n",
    "    entities = [entity.text for entity in spacy_doc.ents \n",
    "                if not entity.label_ in remove]\n",
    "\n",
    "    num_entities = len(set(entities))\n",
    "    words_sent = num_words / num_sents\n",
    "    char_word = characters / num_words\n",
    "    lex_div = vocab / num_words\n",
    "    \n",
    "    stats = [num_sents, num_tokens, num_words, vocab, num_entities, \n",
    "             words_sent, char_word, lex_div]\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AcjtNHJ1JeLO"
   },
   "source": [
    "### Read the CNN Lite plain text file articles into a corpus using the NLTK's PlaintextCorpusReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clvA9lkNJeLP"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"PATH = \\\"cnn_lite_corpus/\\\"\\nDOC_PATTERN = r\\\".*\\\\.txt\\\"\\ncnn_corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)\";\n",
       "                var nbb_formatted_code = \"PATH = \\\"cnn_lite_corpus/\\\"\\nDOC_PATTERN = r\\\".*\\\\.txt\\\"\\ncnn_corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = \"cnn_lite_corpus/\"\n",
    "DOC_PATTERN = r\".*\\.txt\"\n",
    "cnn_corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5KxRP36JeLR"
   },
   "source": [
    "### Iterate through the fileids in the corpus, extract the raw text of each document, and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0BzPgeiJeLR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIF6IXkrJeLT"
   },
   "source": [
    "### Write a function that calculates the following statistics for a document and returns them as a list.\n",
    "\n",
    "- Number of sentences\n",
    "- Number of tokens\n",
    "- Number of words (no stop words or punctuation)\n",
    "- Number of unique words (vocabulary)\n",
    "- Number of unique named entities (excluding numbers, dates, times, and currency types)\n",
    "- Average sentence length\n",
    "- Average word length\n",
    "- Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXImZOGjJeLU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTzNB5ChJeLW"
   },
   "source": [
    "### Iterate through all the documents, calculate these statistics for each one, and store all the results in a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmLnv-FgJeLW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMusidpEJeLY"
   },
   "source": [
    "### Summarize these statistics for the entire corpus by calling the Pandas `describe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaoJyuE4JeLZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ljYdviFFJeLa"
   },
   "source": [
    "### Choose a document from the list of documents you created earlier and generate a frequency distribution bar chart for it showing which terms appear most frequently in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3AuhSQzJeLb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1axjKBypJeLc"
   },
   "source": [
    "### Generate a word cloud visualization for the same document for which you generated the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ErgYM4QJeLd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBsZEhS1JeLf"
   },
   "source": [
    "### Choose a different article (preferably one that references several named entities) and create a dispersion plot that shows the occurrence of those entities throughout the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMcdU5gMJeLf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeiGkRCjJeLg"
   },
   "source": [
    "### Choose another article and generate a POS visualization highlighting the parts of speech for tokens in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQ7zBV-qJeLh"
   },
   "outputs": [],
   "source": [
    "from yellowbrick.text.postag import PosTagVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IbXdOnPJeLi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 73, Lecture 2: Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
