{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Before answering this question, see where you are in the data-processing workflow:\n",
    "\n",
    "![Feature Engineering](assets/feature_engineering.png)\n",
    "\n",
    "As the diagram above shows, with this checkpoint, you'll start to explore how to convert text data into numerical form. This is the feature-engineering step that you need to do before feeding your data into any machine-learning algorithm. Converting text into numerical form is often called *language modeling* in NLP jargon, and it's one of the most active research areas in NLP. This process is called language modeling because of semantics; a good numerical representation of the text should be able to capture the meaning of the words and their relationships between each other. You'll learn about semantics in the next checkpoint. For now, you'll just focus on a simple way to represent words in numerical form.\n",
    "\n",
    "To accommodate the feature-generation techniques that you'll learn here and to see how they perform on a machine-learning task, you'll feed your new numerical features into some machine-learning algorithms. You'll also make some classifications to demonstrate the pros and cons of your feature-engineering methods. Hence, your first hands-on NLP application in this module will be *text classification*. \n",
    "\n",
    "Continue on to get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words\n",
    "\n",
    "The first feature-generation approach that you'll learn about here is called *bag-of-words* (BoW). BoW is quite simple: your goal is to create a feature matrix such that the rows are observations, and each column is a unique word in your vocabulary. You fill in this matrix by counting how many times each word appears in each observation. You then use those counts as features. \n",
    "\n",
    "As mentioned, BoW is simple and very easy to implement using libraries like scikit-learn. In the jargon of scikit-learn, generating BoW features is called `CountVectorizer`, as you'll see shortly. However, before moving on to implement the BoW approach, you'll need to do some data cleaning. \n",
    "\n",
    "Begin by importing the libraries that you'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a helper function called `text_cleaner` for cleaning the text. Specifically, remove some punctuation marks and numbers from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
    "    # recognize: the double dash '--'. Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load Jane Austen's *Persuasion* and Lewis Carroll's *Alice's Adventures in Wonderland* from NLTK's Gutenberg module. In this checkpoint, you'll be working on these two texts, and your ultimate goal will be to distinguish the authors from their sentences. Hence, your unit of observation (your *documents*) will be the sentences of these novels.\n",
    "\n",
    "After you load the novels, do some data cleaning. First, remove the chapter indicators from the novels. Then apply the `text_cleaner` function from above to clean up some punctuation marks and the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned texts are stored in two variables called `alice` and `persuasion`. Note that you haven't split the texts into sentences yet. You'll do that using spaCy. For that purpose, load spaCy's English module and use spaCy to parse both the `alice` and `persuasion` texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take some time.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split your texts into sentences now. This process is easy using spaCy. Because you've already parsed your documents with spaCy, you can now use spaCy's functionalities. In this case, spaCy will take care of deriving the sentences from the texts. What you need to do is to iterate over the parsed documents after calling the `.sents` attribute. With the following code, you can iterate using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "As a result, your dataset consists of two columns. The first column has the sentences, and the second column has the authors. Before jumping into BoW, you need to remove stop words and punctuation marks, and then convert your tokens to lemmas or stems. In this example, you'll lemmatize your tokens. Again, you'll make use of the attributes of the documents that spaCy parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of stop words and punctuation,\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = \" \".join(\n",
    "        [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start converting the text in the first column of your dataset into a numerical form. As mentioned before, you'll use the BoW approach. For this purpose, use `CountVectorizer` from scikit-learn, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all! Now, check out your new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you now have a dataset that matches the format that you're used to in this program. It's in tabular form: observations are the rows and features are the columns. More importantly, you converted text into a numerical form, so you can apply machine-learning algorithms using these as input. This enables you to move to the modeling phase, as indicated below.\n",
    "\n",
    "![Modeling](assets/modeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## BoW in action\n",
    "\n",
    "Now, give the bag-of-words features a whirl by trying some machine-learning algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "It looks like logistic regression and random forest overfit. Overfitting is a known problem when using the bag-of-words approach because it involves throwing a massive number of features at a model. Some of those features (in this case, word frequencies) will capture noise in the training set. Because overfitting is also a known problem with random forests, the divergence between training score and test score is expected. On the other hand, training and test scores from gradient boositng are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams: words in context\n",
    "\n",
    "Consider the word `vain` in these two sentences:\n",
    "\n",
    "    “She labored in vain; the rock would not move.” \n",
    "\n",
    "    “She was so vain, her bathroom mirror was covered in lip prints.”\n",
    "\n",
    "In both sentences, `vain` is an adjective. In the first sentence, it signals a lack of success. In the second sentence, the same word means vanity. Since the two usages can't be distinguished by their part of speech, how can you tell them apart?\n",
    "\n",
    "*N-grams* incorporate context information by creating features made up of a series of consecutive words. The *n* refers to the number of words included in the series. For example, the 2-gram representation of the first sentence would be as follows:\n",
    "\n",
    "    (She labored), (labored in), (in vain), (vain the), (the rock), (rock would), (would not), (not move).\n",
    "\n",
    "The 3-gram representation of the second sentence would be as follows:\n",
    "\n",
    "    (She was so), (was so vain), (so vain her), (vain her bathroom), (her bathroom mirror), (bathroom mirror was), (mirror was covered), (was covered in), (covered in lip), (in lip prints).\n",
    "\n",
    "Each of the word sets could then operate as its own feature. N-grams can be used to create term-document matrices (though it would now be n-gram-document matrices), or they can be used in topic modeling. In addition, n-grams are useful for text prediction; they can be used to determine what words are most likely to follow in a sentence, phrase, or search query.\n",
    "\n",
    "For a sentence with *X* words, there will be $X-(N-1)$ n-grams. Two-gram phrases are also called *bigrams*, three-gram phrases are called *trigrams*, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use single-word models?\n",
    "\n",
    "Given the benefits of incorporating word context for distinguishing between different meanings of a word, why would any NLP practitioner worth their salt ever use simple word features? Well, models based on single words have several advantages:\n",
    "\n",
    "* N-gram models are considerably more sparse than single-word models. The two `vain` sentences above share four words (`she`, `in`, `vain`, `the`) but zero n-grams. Sparseness does mean that an n-gram model can be stored in a more memory-efficient way. For example, imagine a dict that only lists the n-grams that are present in each sentence, rather than a set of columns with `1` if an n-gram is present and `0` otherwise. But it also means that a larger corpus may be needed to detect any shared patterns across documents. In other words, n-gram models may need more documents before they start to give good results.\n",
    "\n",
    "* Single-word models are straightforward to implement, while models incorporating n-grams are more sensitive to fine distinctions of meaning. Which to choose depends on the goals of the NLP project and the tradeoffs in time and performance for the specific corpus that you are modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of 2-grams\n",
    "\n",
    "Implementing n-grams is quite straightforward using scikit-learn's `CountVectorizer`. The only thing that you need to do is to give a tuple of range as values to `ngram_range` (a parameter of `CountVectorizer`). As the code below demonstrates, you need to provide a value for the parameter `ngram_range=(2,2)` inside `CountVectorizer`. This means that the vectorizer will produce 2-gram features. If you were to give `ngram_range=(1,2)` as the value, then the vectorizer would produce both 1-gram and 2-gram features together. But don't do that just yet—that task will be saved for this checkpoint's assignment.\n",
    "\n",
    "Now, generate your 2-grams and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use 2-grams\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, your new features are 2-gram. Next, build the same machine-learning models that you built before for the 1-gram case, but this time, use 2-gram as your features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem worse than 1-gram! Even the overfitting in the logistic regression and the random forest is higher than before. That's because in the 2-gram case, you have more features than you have in 1-gram. One possible solution to increase the performance of the models is using 1-gram and 2-gram together as features. This will be one of your tasks in the assignments."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
