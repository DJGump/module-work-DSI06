{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"colab":{"name":"bfi_factor_analysis_blanks.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"_8mkatT1FU8B","colab_type":"text"},"source":["# Factor Analysis\n","\n","This notebook is based on/extending an article posted originally on [DataCamp](https://www.datacamp.com/community/tutorials/introduction-factor-analysis).  The original article is aging, and its example code hasn't been updated to reflect changes in the `factor_analyzer` package used.  The info and discussion from article still a good resource.\n","\n","The analysis will be focused on the `BFI` dataset; more information on this data can be found [here](https://www.personality-project.org/r/html/bfi.html)."]},{"cell_type":"code","metadata":{"id":"LlASOF9HFU8C","colab_type":"code","colab":{}},"source":["# %reload_ext nb_black"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"McOhAadaFU8G","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.decomposition import PCA, FastICA\n","\n","# !pip install factor_analyzer\n","from factor_analyzer import FactorAnalyzer\n","from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","data_url = \"https://vincentarelbundock.github.io/Rdatasets/csv/psych/bfi.csv\"\n","bfi = pd.read_csv(data_url, index_col=0)\n","\n","bfi.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"QdD_00zNFU8J","colab_type":"text"},"source":["Our features of interest for this analysis will be the responses to the questions.  These are all the features excluding the subject id and demographic information."]},{"cell_type":"code","metadata":{"id":"wltMd1r4FU8J","colab_type":"code","colab":{}},"source":["# fmt: off\n","feat_names = ['A1', 'A2', 'A3', 'A4', 'A5', \n","              'C1', 'C2', 'C3', 'C4', 'C5',\n","              'E1', 'E2', 'E3', 'E4', 'E5', \n","              'N1', 'N2', 'N3', 'N4', 'N5', \n","              'O1', 'O2', 'O3', 'O4', 'O5']\n","# fmt: on\n","\n","X = bfi[feat_names]\n","\n","print(\"Top missing value features for X:\")\n","print(X.isnull().mean().sort_values(ascending=False).head())\n","print(f\"\\nX shape: {X.shape}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"XkqBmi-vFU8M","colab_type":"text"},"source":["Dropping our missing values doesn't impact too much in light of our missing value rate and sample size.  We'll just drop them without too much thought in this demo."]},{"cell_type":"code","metadata":{"id":"Yu5NWW0HFU8M","colab_type":"code","colab":{}},"source":["X = X.dropna()\n","print(f\"X shape: {X.shape}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"JyG_VrvBFU8P","colab_type":"text"},"source":["If we wanted to make predictions with this data using a linear model (or something similiar) we'd want to check for multicollinearity.  Let's make a heatmap and see if we spot some potentially assumption breaking correlations.\n","\n","From the heatmap, we see some highly correlated blocks.  The most obvious examples obvious examples are the `N*` features and the `A*` features."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"LDtFFsDFFU8P","colab_type":"code","colab":{}},"source":["sns.heatmap(X.corr())\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"PKiaBSmCFU8S","colab_type":"text"},"source":["If we wanted to proceed with building a model, we might want to do some dimension reduction first.  Let's use factor analysis through the `factor_analyzer` package.  This package implements the commonly used `varimax` rotation [unlike `sklearn`](https://github.com/scikit-learn/scikit-learn/issues/2688) (`varimax` appears to be [under development still for `sklearn`](https://github.com/scikit-learn/scikit-learn/pull/11064) as of writing this).\n","\n","Before running a factor analysis, we need to do some \"adequacy tests\".  If these tests fail, we should not run a factor analysis.\n","\n","> * Bartlettâ€™s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis.\n","> * Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate."]},{"cell_type":"code","metadata":{"id":"FG4yN27lFU8S","colab_type":"code","colab":{}},"source":["_, p_value = calculate_bartlett_sphericity(X)\n","print(f\"* Passed Bartlett adequacy test?\\n\\t{p_value < 0.05} (p = {p_value:.4f})\")\n","\n","_, kmo = calculate_kmo(X)\n","print(f\"* Passed Kaiser-Meyer-Olkin adequacy test?\\n\\t{kmo >= 0.6} (kmo = {kmo:.4f})\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"hpcDAONQFU8V","colab_type":"text"},"source":["Cool, we passed our tests. Lets get to factor analyzing.\n","\n","But how many factors should we choose?  One way to do this is with the [Kaiser Criterion](https://en.wikipedia.org/wiki/Factor_analysis#Older_methods), which is nice due to its simplicity.  We calclulate some eigenvalues and count how many are above 1.  Other [more 'modern' methods](https://en.wikipedia.org/wiki/Factor_analysis#Modern_criteria) are harder to find implementations for."]},{"cell_type":"code","metadata":{"id":"efO84XZqFU8V","colab_type":"code","colab":{}},"source":["# This will throw a FutureWarning\n","# (factor_analyzer version at time of writing this is 0.3.2)\n","fa = FactorAnalyzer(n_factors=25, rotation=\"varimax\")\n","_ = fa.fit_transform(X)\n","ev, _ = fa.get_eigenvalues()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4ipGmFqFU8Y","colab_type":"text"},"source":["`matplotlib` practice!  Plot the eigenvalues (stored in `ev`)\n","\n","* [ ] your x axis should be the range [1, # of eigenvalues]\n","* [ ] your y axis should be the eigenvalue\n","* [ ] plot a horizontal line at `y=1`\n","  * [ ] make the line dashed\n","  * [ ] make the line black\n","* [ ] for eigenvalues > 1, make the scatter marker a different color\n","* [ ] give the plot meaningful axis labels and a title"]},{"cell_type":"code","metadata":{"id":"CuEp-zLvFU8Y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nl9EMLGFFU8b","colab_type":"text"},"source":["From our candidate 25 factors, 6 meet the cutoff value of 1.  The 6th is pretty close to our cutoff, but we'll keep it in the set until it's proven not to be useful."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"dmbkuQZEFU8b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"5db6c53e-2118-46c7-9708-63d30dde80d4","executionInfo":{"status":"ok","timestamp":1591186180701,"user_tz":240,"elapsed":2076,"user":{"displayName":"Adam Spannbauer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0dsIFeXnf3iRsUkFzJFh96GSrIp79KYpNuPtB=s64","userId":"04097551985177324740"}}},"source":["# This will throw a FutureWarning\n","# (factor_analyzer version at time of writing this is 0.3.2)\n","fa = FactorAnalyzer(n_factors=6, rotation=\"varimax\")\n","X_transformed = fa.fit_transform(X)\n","\n","print(X.shape)\n","print(X_transformed.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(2436, 25)\n","(2436, 6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pVQBCTrZFU8d","colab_type":"text"},"source":["We successfully trimmed down our feature set to 6... so what?  We could do that with PCA or a lot of other methods. \n","\n","Let's look at the factor `loadings_` to see how the original features map to our reduced factors.  We'll use some `pandas` styling to highlight the magnitude of each feature's contribution to the factor (note that this is the *magnitude* aka absolute value)."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"pU72uaQpFU8e","colab_type":"code","colab":{}},"source":["# Use the loadings_ attribute to build a dataframe \n","# Make the dataframe row names the orignal feature names\n","\n","\n","# Display the loadings with each cell colored by the value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4Jq4WuIFU8g","colab_type":"text"},"source":["Our 6th factor isn't adding too much since our first 5 are covering pretty much all the questions.  This factor also doesn't have the benefit of being interpretable like the others.  Recall, it was also borderline with our eigenvalue > 1 rule-of-thumb; let's drop it and move on."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"aFCoypKaFU8g","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WA9q1YVFU8j","colab_type":"text"},"source":["From our factor analysis we reduced our dimensions and still kept a high level of intpretability in the terms of this dataset.  We can look at another correlation heatmap to see if our data is now green lit for a model that assumes no multicollinearity."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"oA53o3K2FU8j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YFckuDYFU8l","colab_type":"text"},"source":["Let's also quickly throw `PCA` at our data to contrast the 2 methods.\n","\n","From our output we see that unlike factor analysis, the loadings don't offer an easy interpretation."]},{"cell_type":"code","metadata":{"id":"VtAkmVUhFU8m","colab_type":"code","colab":{}},"source":["def pca_loadings(pca):\n","    return pca.components_.T * np.sqrt(pca.explained_variance_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVrT9eabFU8o","colab_type":"code","colab":{}},"source":["# Perform pca with 5 components and create the same output as for FA\n","pca = \n","\n","loadings = pca_loadings(pca)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eM5tUvCEFU8r","colab_type":"text"},"source":["Let's also throw ICA in the mix."]},{"cell_type":"code","metadata":{"id":"FiiToys4FU8r","colab_type":"code","colab":{}},"source":["ica = FastICA(n_components=5)\n","nba_ics = ica.fit_transform(X)\n","\n","loadings = pd.DataFrame(ica.mixing_)\n","loadings.index = X.columns\n","loadings.columns = [f\"ind_component_{i}\" for i in range(loadings.shape[1])]\n","\n","# # Sort by absolute value of column\n","# sort_order = loadings.abs().sort_values(\"ind_component_0\", ascending=False).index\n","# loadings = loadings.reindex(sort_order)\n","\n","print(\"Component loadings absolute values (colored by magnitude)\")\n","loadings.abs().style.background_gradient(axis=None)"],"execution_count":0,"outputs":[]}]}