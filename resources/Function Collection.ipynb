{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_summary(df, print_log=False, sort='none'):\n",
    "    summary = df.apply(lambda x: x.isna().sum() / x.shape[0])\n",
    "    \n",
    "    if print_log == True:\n",
    "        if sort == 'none':\n",
    "            print(summary)\n",
    "        elif sort == 'ascending':\n",
    "            print(summary.sort_values())\n",
    "        elif sort == 'descending':\n",
    "            print(summary.sort_values(ascending=False))\n",
    "        else:\n",
    "            print('Invalid value for sort parameter.')\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vif(x):\n",
    "    import warnings\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \"\"\"Utility for checking multicollinearity assumption\n",
    "    \n",
    "    :param x: input features to check using VIF. This is assumed to be a pandas.DataFrame\n",
    "    :return: nothing is returned the VIFs are printed as a pandas series\n",
    "    \"\"\"\n",
    "    # Silence numpy FutureWarning about .ptp\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        x = sm.add_constant(x)\n",
    "\n",
    "    vifs = []\n",
    "    for i in range(x.shape[1]):\n",
    "        vif = variance_inflation_factor(x.values, i)\n",
    "        vifs.append(vif)\n",
    "\n",
    "    print(\"VIF results\\n-------------------------------\")\n",
    "    print(pd.Series(vifs, index=x.columns))\n",
    "    print(\"-------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a function, but this is handy\n",
    "cat_cols = [\"season\", \"weather\", \"weekday\", \"hour\"]\n",
    "drop_cats = [1, 1, \"Saturday\", 0]\n",
    "\n",
    "num_cols = [\"temp\", \"humidity\", \"windspeed\"]\n",
    "\n",
    "bin_cols = [\"holiday\"]\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"one_hot_encode\", OneHotEncoder(drop=drop_cats, sparse=False), cat_cols),\n",
    "        (\"scale\", StandardScaler(), num_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "ct.fit(X_train)\n",
    "\n",
    "X_train_trans = ct.transform(X_train)\n",
    "X_test_trans = ct.transform(X_test)\n",
    "\n",
    "cat_names = ct.transformers_[0][1].get_feature_names(cat_cols)\n",
    "cat_names = list(cat_names)\n",
    "new_col_names = cat_names + num_cols + bin_cols\n",
    "\n",
    "X_train = pd.DataFrame(X_train_trans, columns=new_col_names)\n",
    "X_test = pd.DataFrame(X_test_trans, columns=new_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preds(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "\n",
    "    rmse = np.sqrt((error ** 2).mean())\n",
    "    mae = error.abs().mean()\n",
    "    mape = (error / y_true).abs().mean()\n",
    "\n",
    "    print(f\"rmse {rmse:.2f}\")\n",
    "    print(f\"mae {mae:.2f}\")\n",
    "    print(f\"mape {mape:.2f}\")\n",
    "\n",
    "    line_pts = [y_true.min(), y_true.max()]\n",
    "    plt.scatter(y_true, y_pred)\n",
    "    plt.plot(line_pts, line_pts, c=\"red\", ls=\"--\", alpha=0.5)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Fit\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], c=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve (AUC is {auc:.4f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(train_columns, model, modeltype_maybe):\n",
    "    '''\n",
    "    Write a function that chooses important features based on an importance threshold.\n",
    "    Will need to work for a variety of estimator objects, eg a RandomForestClassifier and a GridSearchCV\n",
    "    fit as a DecisionTreeRegressor.\n",
    "    '''\n",
    "\n",
    "    # inspiration code, keep handy\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": X_train.columns,\n",
    "            \"importance\": forest1.best_estimator_.feature_importances_,\n",
    "        }\n",
    "    ).sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(X, Y, h):\n",
    "    # Our data. Converting from data frames to arrays for the mesh.\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Mesh size.\n",
    "    h = 4.0\n",
    "\n",
    "    # Plot the decision boundary. We assign a color to each point in the mesh.\n",
    "    x_min = X[:, 0].min() - .5\n",
    "    x_max = X[:, 0].max() + .5\n",
    "    y_min = X[:, 1].min() - .5\n",
    "    y_max = X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h),\n",
    "        np.arange(y_min, y_max, h)\n",
    "    )\n",
    "    Z = neighbors.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(6, 4))\n",
    "    plt.set_cmap(plt.cm.Paired)\n",
    "    plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "    # Add the training points to the plot.\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "    plt.xlabel('Loudness')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.title('Mesh visualization')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column names back after selectkbest\n",
    "\n",
    "feature_names = list(X.columns.values)\n",
    "\n",
    "mask = selector.get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "        \n",
    "best_data = pd.DataFrame(X_best, columns=new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a42f5573c511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Make importances relative to max importance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeature_importance\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfeature_importance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msorted_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "\n",
    "# Replace with whatever model import(s) you're using\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Fill out your column datatypes here\n",
    "num_cols = []\n",
    "\n",
    "bin_cols = []\n",
    "\n",
    "cat_cols = []\n",
    "drop_cats = []\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    # Should only use one of these\n",
    "    # Comment out or delete one of the below 2 lines\n",
    "    ('OneHotEncoder', OneHotEncoder(drop=drop_cats), cat_cols),    \n",
    "    ('leaveoneoutencoder', LeaveOneOutEncoder(), cat_cols),\n",
    "\n",
    "    # Scale numeric columns (not needed for all models but can't hurt)\n",
    "    ('scaler', StandardScaler(), num_cols)\n",
    "    \n",
    "    # bin_cols we'll leave untouch\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    # Choose your model and put it here\n",
    "    ('model', XGBClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "grid = {\n",
    "    # Use model__ with hyperprammeter names after\n",
    "    'model__n_estimators':[100, 150]    \n",
    "}\n",
    "\n",
    "pipeline_cv = GridSearchCV(pipeline, grid)\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "print(pipeline_cv.score(X_train, y_train))\n",
    "print(pipeline_cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "# Replace with whatever model import(s) you're using\n",
    "from xgboost import XGBClassifier\n",
    "# Fill out your column datatypes here\n",
    "num_cols = []\n",
    "bin_cols = []\n",
    "cat_cols = []\n",
    "drop_cats = []\n",
    "preprocessing = ColumnTransformer([\n",
    "    # Should only use one of these\n",
    "    # Comment out or delete one of the below 2 lines\n",
    "    ('OneHotEncoder', OneHotEncoder(drop=drop_cats), cat_cols),\n",
    "    ('leaveoneoutencoder', LeaveOneOutEncoder(), cat_cols),\n",
    "    # Scale numeric columns (not needed for all models but can't hurt)\n",
    "    ('scaler', StandardScaler(), num_cols)\n",
    "    # bin_cols we'll leave untouch\n",
    "], remainder='passthrough')\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    # Choose your model and put it here\n",
    "    ('model', XGBClassifier())\n",
    "])\n",
    "grid = {\n",
    "    # Use model__ with hyperprammeter names after\n",
    "    'model__n_estimators':[100, 150]\n",
    "}\n",
    "pipeline_cv = GridSearchCV(pipeline, grid)\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "print(pipeline_cv.score(X_train, y_train))\n",
    "print(pipeline_cv.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
