{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances and Disimilarities Cheat Sheet\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "* [Numeric Distances](#1.-Numeric-Distances)\n",
    "    * [Manhattan](#1.1-Manhattan-distance)\n",
    "    * [Euclidean](#1.2-Euclidean-distance)\n",
    "    * [Chebyshev](#1.3-Chebyshev-distance)\n",
    "    * [Minkowski](#1.4-Minkowski-(General))\n",
    "    * [Cosine Similarity](#1.5-Cosine-Similarity)\n",
    "* [Categorical Distances](#2.-Categorical-Distances)\n",
    "    * [Hamming](#2.1-Hamming-Distance)\n",
    "    * [Dice](#2.2-Dice-dissimilarity)\n",
    "    * [Jaccard](#2.3-Jaccard-distance)\n",
    "* [Mixed Distances](#3.-Distances-for-mixed-data)\n",
    "    * [Gower](#Gower.Gower-Gower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Numeric Distances\n",
    "1.1 Manhattan (L<sub>1</sub>)\n",
    "\n",
    "1.2 Euclidean (L<sub>2</sub>)\n",
    "\n",
    "1.3 Chebyshev(L<sub>$\\infty$</sub>)\n",
    "\n",
    "1.4 Minkowski (General L<sub>p</sub>)\n",
    "\n",
    "1.5 Cosine Similarity\n",
    "\n",
    "#### 1.1 Manhattan distance\n",
    "\n",
    "$$\\sum_{i=0}^n|x_i - y_i|$$\n",
    "\n",
    "* Intuition: \"Taxi cab/city block distance\" This metric will be less affected by outlier differences in the calculation than euclidean.\n",
    "* Examples:\n",
    "    * `manhattan([0,0], [3,4])` is 7\n",
    "    * `manhattan([0,0], [3,10])` is 13\n",
    "    * `manhattan([2,3], [4,6])` is 5\n",
    "* Code: `pdist(x)` or `pdist(x, metric='euclidean')`\n",
    "* \"$L_1$ norm\" (is minkowski distance ($L_p$) with $p=1$)\n",
    "<center>Manhattan (L<sub>1</sub>)</center>\n",
    "<img src='https://i.imgur.com/pvk6uO0.png'>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### 1.2 Euclidean distance\n",
    "\n",
    "$$\\sqrt{\\sum_{i=0}^n(x_i - y_i)^2}$$\n",
    "\n",
    "* Intuition: \"Straight line distance.\" This metric will be more affected by outlier differences in the calculation than Manahattan (due to being squared).\n",
    "* Examples:\n",
    "    * `euclidean([0,0], [3,4])` is 5\n",
    "    * `euclidean([0,0], [3,10])` is 10.44\n",
    "    * `euclidean([2,3], [4,6])` is 3.606\n",
    "* Code: `pdist(x)` or `pdist(x, metric='euclidean')`\n",
    "* \"$L_2$ norm\" (is minkowski distance ($L_p$) with $p=2$)\n",
    "<center>Euclidean (L<sub>2</sub>)</center>\n",
    "<img src='https://i.imgur.com/zcuxxY7.png'>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### 1.3 Chebyshev distance\n",
    "\n",
    "$$max(|x_i - y_i|)$$\n",
    "\n",
    "* Intuition: \"The biggest difference between the 2 rows.\" This metric is only affected by outlier differences in the calculation.  (it's only the max)\n",
    "* Examples:\n",
    "    * `chebyshev([0,0], [3,4])` is 4\n",
    "    * `chebyshev([0,0], [3,10])` is 10\n",
    "    * `chebyshev([2,3], [4,6])` is 3\n",
    "* Code: `pdist(x)` or `pdist(x, metric='euclidean')`\n",
    "* \"$L_\\infty$ norm\" (is minkowski distance ($L_p$) with $p=\\infty$)\n",
    "<center>Chebyshev (L<sub>$\\infty$</sub>)</center>\n",
    "<img src='https://i.imgur.com/u1xZkja.png'>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### 1.4 Minkowski (General)\n",
    "\n",
    "All the above distances are versions of minkowski.  Plug in $p=1$ and $p=2$ to prove that's true ($p=\\infty$ is a little tougher to prove).\n",
    "\n",
    "$$\\sqrt[p]{\\sum_{i=0}^n|x_i - y_i|^p}$$\n",
    "\n",
    "* As $p$ gets larger the greater the focus is on the biggest difference between $x$ and $y$.\n",
    "    * In manahattan, $p=1$ and we weight each absolute difference the same.  For example, if we compare `[0, 0]` to `[3, 4]`, the differences are $3$ and $4$ and we simply add them up to get a distance of $7$.\n",
    "    * In euclidean, $p=2$ and by squaring each difference we put a greater emphasis on larger differences.  For example, if we compare `[0, 0]` to `[3, 4]`, the differences are $2$ and $4$.  Squaring these leads to $2^2 = 4$ and $4^2 = 16$; this exagerates the importance of the larger difference and the final result is \n",
    "    * In chebyshev, $p=\\infty$ and we *only* care about the biggest difference\n",
    "<center>Minkowski (L<sub>p</sub>)</center>\n",
    "<img src='https://i.imgur.com/u1xZkja.png'>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### 1.5 Cosine Similarity\n",
    "\n",
    "Cosine similarity ranges from [-1, 1]; to convert this to a 'distance' we do 1 - cosine similarity.  So the new range is [2, 0].\n",
    "\n",
    "$$cos(\\theta) = \\frac{x \\cdotp y}{||x|| ||y||}$$\n",
    "\n",
    "See [this YouTube playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) for a deeper intuition of vectors and linear algebra.\n",
    "\n",
    "* Intuition: \"Angle between the vectors defined by each observation.\"  Focuses more on how each column relates to one another within each observation; if their relationships between columns are similar then this is a small distance.\n",
    "* Examples:\n",
    "    * `cosine_dis([0,0], [3,4])` is nan*\n",
    "    * `cosine_dis([0,0], [3,10])` is nan*\n",
    "    * `cosine_dis([0,0,1], [3,10,1])` is 0.904\n",
    "    * `cosine_dis([2,3], [4,6])` is 0\n",
    "* Code: `pdist(x, metric='cosine')`\n",
    "\n",
    "*think about our plot below; we can't really draw a vector from (0, 0) to (0, 0) and measure the angle between that and another vector\n",
    "<center>Cosine Disimilarity</center>\n",
    "<img src='https://i.imgur.com/5tljRAL.png'>\n",
    "<center>A: [1,1], B: [4,4], C:[5,9]</center>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Categorical Distances\n",
    "\n",
    "2.1. Hamming Distance(0's are meaningful)\n",
    "\n",
    "2.2. Dice Disimilarity (matching 1's is much more important)\n",
    "\n",
    "2.3. Jacard Distance (Middle ground for binary and dummy)\n",
    "\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### 2.1 Hamming Distance\n",
    "\n",
    "$$\\frac{n_{misses}}{n_{columns}}$$\n",
    "\n",
    "* **Makes a lot of sense for binary columns where a `0` is a meaningful response.**\n",
    "* Intuition: \"What fraction of the elements between the 2 rows are differnt?\"\n",
    "* Examples:\n",
    "    * `hamming([0,0,0], [1,1,1])` is $\\frac{3}{3}$ = 1\n",
    "    * `hamming([1,0,0], [1,1,1])` is $\\frac{2}{3}$\n",
    "    * `hamming([1,1,0], [1,1,1])` is $\\frac{1}{3}$\n",
    "    * `hamming([1,1,1], [1,1,1])` is $\\frac{0}{3}$ = 0\n",
    "    * `hamming([0,0,1], [0,0,0])` is $\\frac{1}{3}$\n",
    "    * `hamming([0,0,1], [0,1,1])` is $\\frac{1}{3}$\n",
    "* Code: `pdist(x, metric='hamming')` or `pdist(x, metric='matching')`\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### 2.2 Dice dissimilarity\n",
    "\n",
    "$$\\frac{n_{misses}}{2n_{one\\_matches} + n_{misses}}$$\n",
    "\n",
    "* **Makes a lot of sense for dummy columns where a `0` is a less meaningful response, but matching on a 1 means a lot (i.e. a dummy matching on 1 means the original input categorical data matched).**\n",
    "* Intuition: \"Hamming distance but... ignore matches of `0`s and extra count matches of `1`s\"\n",
    "* Examples:\n",
    "    * `dice([0,0,0], [1,1,1])` is $\\frac{3}{2(0) + 3}$ = 1\n",
    "    * `dice([1,0,0], [1,1,1])` is $\\frac{2}{2(1) + 2}$ = $\\frac{1}{2}$\n",
    "    * `dice([1,1,0], [1,1,1])` is $\\frac{1}{2(2) + 1}$ = $\\frac{1}{5}$\n",
    "    * `dice([1,1,1], [1,1,1])` is $\\frac{0}{2(3) + 0}$ = 0\n",
    "    * `dice([0,0,1], [0,0,0])` is $\\frac{1}{2(0) + 1}$ = 1\n",
    "    * `dice([0,0,1], [0,1,1])` is $\\frac{1}{2(1) + 1}$ = $\\frac{1}{3}$\n",
    "* Code: `pdist(x, metric='dice')`\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### 2.3 Jaccard distance\n",
    "\n",
    "$$\\frac{n_{misses}}{n_{one\\_matches} + n_{misses}}$$\n",
    "\n",
    "* **Makes a lot of sense for a mix of binary and dummy columns**\n",
    "* Intuition: \"What if there was a middle ground between hamming and dice?\"\n",
    "* Examples:\n",
    "    * `jaccard([0,0,0], [1,1,1])` is $\\frac{3}{0 + 3}$ = 1\n",
    "    * `jaccard([1,0,0], [1,1,1])` is $\\frac{2}{1 + 2}$ = $\\frac{2}{3}$\n",
    "    * `jaccard([1,1,0], [1,1,1])` is $\\frac{1}{2 + 1}$ = $\\frac{1}{3}$\n",
    "    * `jaccard([1,1,1], [1,1,1])` is $\\frac{0}{0 + 3}$ = 0\n",
    "    * `jaccard([0,0,1], [0,0,0])` is $\\frac{1}{0 + 1}$ = 1\n",
    "    * `jaccard([0,0,1], [0,1,1])` is $\\frac{1}{1 + 1}$ = $\\frac{1}{2}$\n",
    "* Code: `pdist(x, metric='jaccard')`\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Distances for mixed data\n",
    "\n",
    "3.1. Gower\n",
    "\n",
    "\" 'p much all gower\" -Adam Spannbauer\n",
    "<hr>\n",
    "\n",
    "#### Gower.Gower Gower\n",
    "Gower distance is essentially a combination of manhattan distance and jaccard distance. \n",
    "* It applies manhattan to continuous variables and jaccard to binary variables.\n",
    "    * gower is restrictive in how it preprocesses\n",
    "* With this metric we also have the ability to assign weights to show how important each feature should be in the distance calculation.\n",
    "    \n",
    "```python\n",
    "#!pip install gower\n",
    "import gower\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": [21, 24, 35, 52, 55],\n",
    "        \"account_age\": [2, 3, 12, 20, 18],\n",
    "        \"region\": [\"west\", \"south\", \"west\", \"east\", \"east\"],\n",
    "        \"late_payments\": [\"y\", \"n\", \"y\", \"n\", \"y\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "pd.DataFrame(gower.gower_matrix(df)).style.background_gradient()\n",
    "\n",
    "# I think late_payments should be 5 times as important as the\n",
    "# rest of the features (idk why, just made it up to use weights)\n",
    "\n",
    "# cant use list, use a np.array or a pd.series\n",
    "w = np.array([1, 1, 1, 5])\n",
    "pd.DataFrame(gower.gower_matrix(df, weight=w))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td>Gower Matrix</td><td>Weighted Gower Matrix</td></tr><tr><td><img src='https://i.imgur.com/F1H5qnJ.png'></td><td><img src='https://i.imgur.com/v3xi1Na.png2'></td></tr></table>\n",
    "   \n",
    "Compare the 2 outputs.\n",
    "\n",
    "[0, 2, 4] all had the same value for the more heavily weighted late_payments feature. The distances between [0, 2], [0, 4], and [2, 4] all got smaller when we weighted that feature.\n",
    "\n",
    "[1, 3] had the same value for late_payments feature. The distances between [1, 3] got smaller.\n",
    "\n",
    "We also see larger distances between these 2 groups ([0, 2, 4] <-> [1, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
